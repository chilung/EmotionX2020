{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1-0686028.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chilung/EmotionX2020/blob/master/lab1_0686028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsQ_pHO3cG1m"
      },
      "source": [
        "#Recommend Similar News Articles\n",
        "This notebook demonstrates how to use bag-of-word vectors and cosine similarity for news article recommendation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwgK3j4EDtE",
        "colab_type": "code",
        "outputId": "b166c52f-1058-4bea-97fc-c741a3be4e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "\n",
        "logging.debug('Hello Debug')\n",
        "logging.info('Hello Info')\n",
        "logging.warning('Hello Warning')\n",
        "logging.error('Hello Error')\n",
        "logging.critical('Hello Critical')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Info\n",
            "Hello Warning\n",
            "Hello Error\n",
            "Hello Critical\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B80ijUN40QPr",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torchvision\n",
        "import heapq\n",
        "import unicodedata\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJH61jnWXdsh"
      },
      "source": [
        "#Fetching the Corpus\n",
        "`get_corpus()` reads the CSV file, and then return a list of the news headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TwPf9e26O9sn",
        "colab": {}
      },
      "source": [
        "num_of_documents = 5354\n",
        "\n",
        "def get_corpus():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters\n",
        "  logging.debug(\"Dataset columns: {}\".format(df.columns))\n",
        "  logging.debug(\"Dataset size: {}\".format(len(df)))\n",
        "  corpus = df.title.to_list()[0:num_of_documents]\n",
        "  return corpus\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus = get_corpus()\n",
        "  #for i, c in enumerate(test_corpus):\n",
        "  #  logging.debug(\"document {}: {}\".format(i+1, c))\n",
        "  logging.debug(\"len of corpus: {}\".format(len(test_corpus)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPd927zkgaT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lowerCase(inputStr):\n",
        "    return inputStr.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p7Rk42ePX8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request as urllib\n",
        "\n",
        "def get_stop_words(stop_words_source):\n",
        "  stop_words_byte = urllib.urlopen(stopwords_url)\n",
        "\n",
        "  stop_words = []\n",
        "  for line in stop_words_byte:\n",
        "    stop_words = stop_words + [w for w in line.decode(\"utf-8\").split('\\n') if w != '']\n",
        "  stop_words = [''] + stop_words\n",
        "  return(stop_words)\n",
        "\n",
        "stopwords_url = \"https://bit.ly/nlp-stopwords\"\n",
        "stop_words = get_stop_words(stopwords_url)\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  logging.debug(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QPWShPMeJTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_on_space(document):\n",
        "  return re.split(r' ', document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgxac4MpeJgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(words):\n",
        "  return [w for w in words if w not in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF0VvZNVeJmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_on_punctation(words):\n",
        "  return [re.split(r'\\W+', w)[0] for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "arntcI3OTHTx",
        "colab": {}
      },
      "source": [
        "def tokenize(document):\n",
        "  logging_level = logging.getLogger().getEffectiveLevel()\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "  \n",
        "  words = split_on_space(document) # remain the words like you're unchanged so that we can remove them out in the following process\n",
        "  logging.debug(\"Step 1 split on space: {}\".format(words))\n",
        "\n",
        "  words = [lowerCase(w) for w in words]\n",
        "  logging.debug(\"Step 2 lower case: {}\".format(words))\n",
        "\n",
        "  words = remove_stop_words(words) # the first process of removing stop word such as you're in \n",
        "  logging.debug(\"Step 3 1st remove stop words: {}\".format(words))\n",
        "  \n",
        "  words = split_on_punctation(words) # remove punctation words\n",
        "  logging.debug(\"Step 4 split on punctation: {}\".format(words))\n",
        " \n",
        "  words = remove_stop_words(words) # the first process of removing stop word such as you're in \n",
        "  logging.debug(\"Step 5 2nd remove stop words: {}\".format(words))\n",
        "  \n",
        "  logging.getLogger().setLevel(logging_level)\n",
        "  return words\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_str = \"you'll you'd me my myself I am here. But ,[]\\& is strange to me??!! How do you think\"\n",
        "  logging.debug(test_str)\n",
        "  logging.debug(tokenize(test_str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4lh_w1HXU14"
      },
      "source": [
        "#Computing word frequencies\n",
        "`get_vocab(corpus)` computes the word frequencies in a given corpus. It also collect word frequencies of individual document in the given corpus. It returns two items. The first item is a list of 2-tuples, `vocab`: each tuple contains the token and its frequency. The second item is two dimension list, `doc_vocab`: a list of individual document's list of 2-tuples, each tuple contains the token and its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJvtr3VZLkAE",
        "colab": {}
      },
      "source": [
        "def get_vocab(corpus):\n",
        "  vocabulary = Counter()\n",
        "  doc_vocab = {}\n",
        "  for document in corpus:\n",
        "    logging_level = logging.getLogger().getEffectiveLevel()\n",
        "    logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "    logging.debug(\"origin document: {}\".format(document))\n",
        "    tokens = tokenize(document)\n",
        "    logging.debug(\"token: {}\".format(tokens))\n",
        "    doc_vocab_collection = Counter()\n",
        "    doc_vocab_collection.update(tokens)\n",
        "    doc_vocab_collection['LENGTH'] = len(tokens)\n",
        "    \n",
        "    doc_vocab[document] = doc_vocab_collection\n",
        "    vocabulary.update(tokens)\n",
        "  \n",
        "  logging.getLogger().setLevel(logging_level)\n",
        "  return vocabulary, doc_vocab\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus)\n",
        "  logging.debug(\"vocab: {}\".format(test_vocab))\n",
        "  logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMccoBnnag2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# 'cal_vocab_idf' calculates the idf of each word.\n",
        "# The idf of word is given by log of the result of total number of documents divided by the number of documents in which the word happens.\n",
        "#\n",
        "\n",
        "def cal_vocab_idf(vocab, doc_vocab):\n",
        "  vocab_idf = Counter()\n",
        "  for i, token in enumerate(vocab):\n",
        "    word, freq = token\n",
        "    vocab_idf[word] = 0\n",
        "    for document in doc_vocab:\n",
        "      if word in doc_vocab[document]:\n",
        "        vocab_idf[word] = vocab_idf[word] + 1\n",
        "    vocab_idf[word] = math.log(len(doc_vocab) / vocab_idf[word])\n",
        "  return vocab_idf\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  logging.debug(\"corpus: {}\".format(test_corpus))\n",
        "  logging.debug(\"vocabpus: {}\".format(test_vocab))\n",
        "  logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  logging.debug(\"test_vocab_idf: {}\".format(test_vocab_idf))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2tXw2xOTgR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_doc_tfidf_vec(vocab, doc_vocab, vocb_idf):\n",
        "  doc_tfidf_vec = {}\n",
        "  for document in doc_vocab:\n",
        "    doc_tfidf_vec[document] = []\n",
        "    for i, token in enumerate(vocab): \n",
        "      word, freq = token\n",
        "      # doc_vocab[document][word]: the count of the word in this document\n",
        "      # doc_vocab[document]['LENGTH']: the count of total words in this document\n",
        "      # vocb_idf[word]: the word's idf\n",
        "      doc_tfidf_vec[document] = doc_tfidf_vec[document] + [(doc_vocab[document][word] / doc_vocab[document]['LENGTH']) * vocb_idf[word]]\n",
        "  return doc_tfidf_vec\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  #logging.debug(\"corpus: {}\".format(test_corpus))\n",
        "  #logging.debug(\"vocabpus: {}\".format(test_vocab))\n",
        "  #logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))\n",
        "  #logging.debug(\"test_vocab_idf: {}\".format(test_vocab_idf))\n",
        "  \n",
        "  test_doc_tfidf_vec = cal_doc_tfidf_vec(test_vocab, test_doc_vocab, test_vocab_idf)\n",
        "  logging.debug(\"test_doc_tfidf_vec: {}\".format(test_doc_tfidf_vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cwy_Cb-9cYwe"
      },
      "source": [
        "#Compute BoW (Bag-of-Words) Vector\n",
        "`doc_to_vec(doc, vocab)` returns a bag-of-words vector for document `doc`, corresponding to the presence of a word in `vocab`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FLl103a8OLh",
        "colab": {}
      },
      "source": [
        "def doc2vec(doc):\n",
        "  words = tokenize(doc)\n",
        "  return [1 if token in words else 0 for token, freq in vocab]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l6X2fT2H8PAx"
      },
      "source": [
        "Compute the Bag-of-Words vector for each document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9q3bc4T_kzMy"
      },
      "source": [
        "Cosine similarity between two numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3WuWEBWLQBAM",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "  assert len(vec_a) == len(vec_b)\n",
        "  if sum(vec_a) == 0 or sum(vec_b) == 0:\n",
        "    return 0 # hack\n",
        "  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))\n",
        "  a_2 = sum([i*i for i in vec_a])\n",
        "  b_2 = sum([i*i for i in vec_b])\n",
        "  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FEyKg0mkeLm",
        "colab": {}
      },
      "source": [
        "# tfidf_vector = False\n",
        "tfidf_vector = True\n",
        "\n",
        "if tfidf_vector:\n",
        "  #\n",
        "  # calculate the similiarity based on word tfidf vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"tfidf {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc_tfidf_vec[doc_a], doc_tfidf_vec[doc_b])\n",
        "else:\n",
        "  #\n",
        "  # calculate the similiarity based on word bow vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"bow {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc2vec(doc_a), doc2vec(doc_b))\n",
        "  \n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  doc_tfidf_vec = cal_doc_tfidf_vec(test_vocab, test_doc_vocab, test_vocab_idf)\n",
        "\n",
        "  seed_doc = test_corpus[5]\n",
        "  logging.debug('> \"{}\"'.format(seed_doc))\n",
        "  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(test_corpus)]\n",
        "  logging.debug(similarities)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ddRwu-O1f13Q"
      },
      "source": [
        "# Find Similar Documents\n",
        "Find and print the $k$ most similar titles to a given title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a6rIkWUrmhxd",
        "colab": {}
      },
      "source": [
        "def k_similar(seed_id, k):\n",
        "  seed_doc = corpus[seed_id]\n",
        "  print('> \"{}\"'.format(seed_doc))\n",
        "  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus)]\n",
        "  logging.debug(\"===Similiarities: {}\".format(similarities))\n",
        "  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
        "  nearest = [[corpus[id], similarities[id]] for id in top_indices]\n",
        "  print()\n",
        "  for story in reversed(nearest):\n",
        "    print('* \"{}\" ({})'.format(story[0], story[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IgPZe4rxUVPQ"
      },
      "source": [
        "# Test our program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjW8MQXZUmJU",
        "outputId": "87127141-07d5-4a2a-d01c-b419327eb20c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "corpus = get_corpus()\n",
        "vocab, doc_vocab = get_vocab(corpus)\n",
        "vocab = vocab.most_common(100)\n",
        "vocab_idf = cal_vocab_idf(vocab, doc_vocab)\n",
        "doc_tfidf_vec = cal_doc_tfidf_vec(vocab, doc_vocab, vocab_idf)\n",
        "\n",
        "k_similar(28, 5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"U.S. banks’ dismal first quarter may spell trouble for 2016\"\n",
            "\n",
            "* \"U.S. banks’ dismal first quarter may spell trouble for 2016\" (1.0)\n",
            "* \"U.S. banks’ post-election rally may be just an appetizer\" (0.8155447390056827)\n",
            "* \"U.S. small cap funds shine in first quarter but may have harder road ahead\" (0.7901765024263359)\n",
            "* \"Small banks rally pauses but may not be over yet\" (0.7882341711625722)\n",
            "* \"Biggest U.S. banks clear first hurdle in Fed’s annual stress tests\" (0.7507645888142283)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSC5PYzr_sSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}