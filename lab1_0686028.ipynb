{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab1-0686028.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chilung/EmotionX2020/blob/master/lab1_0686028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HsQ_pHO3cG1m"
      },
      "source": [
        "#Recommend Similar News Articles\n",
        "This notebook demonstrates how to use bag-of-word vectors and cosine similarity for news article recommendation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAwgK3j4EDtE",
        "colab_type": "code",
        "outputId": "393f83dc-f1a9-4db0-dc79-7b033b77edc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "\n",
        "logging.debug('Hello Debug')\n",
        "logging.info('Hello Info')\n",
        "logging.warning('Hello Warning')\n",
        "logging.error('Hello Error')\n",
        "logging.critical('Hello Critical')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Info\n",
            "Hello Warning\n",
            "Hello Error\n",
            "Hello Critical\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B80ijUN40QPr",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torchvision\n",
        "import heapq\n",
        "import unicodedata\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzDvQqw4J4d9",
        "colab_type": "text"
      },
      "source": [
        "#The Implementation of Corpus in Title or Content\n",
        "1.   use Contents instead of Titles. The works will be: (1) modify get_corpus to return corpus, a list of titles and corpus_collection, collecting (title, content) pairs. (2) modify get_vocab to get the index from corpus_title and vocab from corpus content. (3) modify doc_similarity's word bow vector by getting content from corpus[doc]. (4) modify k_similar by getting title from corpus_collection.\n",
        "2.   Switch between title and content by just modify the corpus_collection as  (title, content) pairs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tJH61jnWXdsh"
      },
      "source": [
        "#Fetching the Corpus\n",
        "`get_corpus()` reads the CSV file, and then return a list of the news headlines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TwPf9e26O9sn",
        "colab": {}
      },
      "source": [
        "def get_corpus():\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv') # https://bit.ly/nlp-reuters\n",
        "  logging.debug(\"Dataset columns: {}\".format(df.columns))\n",
        "  logging.debug(\"Dataset size: {}\".format(len(df)))\n",
        "  \n",
        "  # num_of_documents = 40\n",
        "  num_of_documents = len(df)\n",
        "  \n",
        "  corpus_title = df.title.to_list()[0:num_of_documents]\n",
        "  corpus_content = df.content.to_list()[0:num_of_documents]\n",
        "\n",
        "  corpus_collection = {}\n",
        "  for i in range(len(corpus_title)):\n",
        "    corpus_collection[corpus_title[i]] = corpus_content[i]\n",
        "    # corpus_collection[corpus_title[i]] = corpus_title[i] # for swithing between title or content, just use this line and comment the previous line\n",
        "  return corpus_title, corpus_collection\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus, test_corpus_collection = get_corpus()\n",
        "  logging.debug(\"len of corpus: {}\".format(len(test_corpus)))\n",
        "\n",
        "  logging.debug(\"corpus {}\".format(test_corpus))\n",
        "  logging.debug(\"corpus_collection: {}\".format(test_corpus_collection))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPd927zkgaT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lowerCase(inputStr):\n",
        "    return inputStr.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p7Rk42ePX8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import urllib.request as urllib\n",
        "\n",
        "def get_stop_words(stop_words_source):\n",
        "  stop_words_byte = urllib.urlopen(stopwords_url)\n",
        "\n",
        "  stop_words = []\n",
        "  for line in stop_words_byte:\n",
        "    stop_words = stop_words + [w for w in line.decode(\"utf-8\").split('\\n') if w != '']\n",
        "  stop_words = [''] + stop_words\n",
        "  return(stop_words)\n",
        "\n",
        "stopwords_url = \"https://bit.ly/nlp-stopwords\"\n",
        "stop_words = get_stop_words(stopwords_url)\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  logging.debug(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QPWShPMeJTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_on_space(document):\n",
        "  return re.split(r' ', document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgxac4MpeJgh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_stop_words(words):\n",
        "  return [w for w in words if w not in stop_words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF0VvZNVeJmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_on_punctation(words):\n",
        "  return [re.split(r'\\W+', w)[0] for w in words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "arntcI3OTHTx",
        "colab": {}
      },
      "source": [
        "def tokenize(document):\n",
        "  logging_level = logging.getLogger().getEffectiveLevel()\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "  \n",
        "  words = split_on_space(document) # remain the words like you're unchanged so that we can remove them out in the following process\n",
        "  logging.debug(\"Step 1 split on space: {}\".format(words))\n",
        "\n",
        "  words = [lowerCase(w) for w in words]\n",
        "  logging.debug(\"Step 2 lower case: {}\".format(words))\n",
        "\n",
        "  words = remove_stop_words(words) # the first process of removing stop word such as you're in \n",
        "  logging.debug(\"Step 3 1st remove stop words: {}\".format(words))\n",
        "  \n",
        "  words = split_on_punctation(words) # remove punctation words\n",
        "  logging.debug(\"Step 4 split on punctation: {}\".format(words))\n",
        " \n",
        "  words = remove_stop_words(words) # the first process of removing stop word such as you're in \n",
        "  logging.debug(\"Step 5 2nd remove stop words: {}\".format(words))\n",
        "  \n",
        "  logging.getLogger().setLevel(logging_level)\n",
        "  return words\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_str = \"you'll you'd me my myself I am here. But ,[]\\& is strange to me??!! How do you think\"\n",
        "  logging.debug(test_str)\n",
        "  logging.debug(tokenize(test_str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4lh_w1HXU14"
      },
      "source": [
        "#Computing word frequencies\n",
        "`get_vocab(corpus)` computes the word frequencies in a given corpus. It also collect word frequencies of individual document in the given corpus. It returns two items. The first item is a list of 2-tuples, `vocab`: each tuple contains the token and its frequency. The second item is two dimension list, `doc_vocab`: a list of individual document's list of 2-tuples, each tuple contains the token and its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QJvtr3VZLkAE",
        "colab": {}
      },
      "source": [
        "def get_vocab(test_corpus_collection):\n",
        "  logging_level = logging.getLogger().getEffectiveLevel()\n",
        "  logging.getLogger().setLevel(logging.INFO)\n",
        "\n",
        "  vocabulary = Counter()\n",
        "  doc_vocab = {}\n",
        "  for title in test_corpus_collection:\n",
        "\n",
        "    logging.debug(\"origin document: {}\".format(test_corpus_collection[title]))\n",
        "    tokens = tokenize(test_corpus_collection[title])\n",
        "    logging.debug(\"token: {}\".format(tokens))\n",
        "    doc_vocab_collection = Counter()\n",
        "    doc_vocab_collection.update(tokens)\n",
        "    doc_vocab_collection['LENGTH'] = len(tokens)\n",
        "    \n",
        "    doc_vocab[title] = doc_vocab_collection\n",
        "    vocabulary.update(tokens)\n",
        "  \n",
        "  logging.getLogger().setLevel(logging_level)\n",
        "  return vocabulary, doc_vocab\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus, test_corpus_collection = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus_collection)\n",
        "  logging.debug(\"vocab: {}\".format(test_vocab))\n",
        "  logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEI9QWoKqMc_",
        "colab_type": "text"
      },
      "source": [
        "#Compute Words' IDF and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiMccoBnnag2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# 'cal_vocab_idf' calculates the idf of each word.\n",
        "# The idf of word is given by log of the result of total number of documents divided by the number of documents in which the word happens.\n",
        "#\n",
        "\n",
        "def cal_vocab_idf(vocab, doc_vocab):\n",
        "  vocab_idf = Counter()\n",
        "  for i, token in enumerate(vocab):\n",
        "    word, freq = token\n",
        "    vocab_idf[word] = 0\n",
        "    for document in doc_vocab:\n",
        "      if word in doc_vocab[document]:\n",
        "        vocab_idf[word] = vocab_idf[word] + 1\n",
        "    vocab_idf[word] = math.log(len(doc_vocab) / vocab_idf[word])\n",
        "  return vocab_idf\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus, test_corpus_collection = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus_collection)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  logging.debug(\"corpus: {}\".format(test_corpus))\n",
        "  logging.debug(\"vocabpus: {}\".format(test_vocab))\n",
        "  logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  logging.debug(\"test_vocab_idf: {}\".format(test_vocab_idf))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2tXw2xOTgR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_doc_tfidf_vec(vocab, doc_vocab, vocb_idf):\n",
        "  doc_tfidf_vec = {}\n",
        "  for document in doc_vocab:\n",
        "    doc_tfidf_vec[document] = []\n",
        "    for i, token in enumerate(vocab): \n",
        "      word, freq = token\n",
        "      # doc_vocab[document][word]: the count of the word in this document\n",
        "      # doc_vocab[document]['LENGTH']: the count of total words in this document\n",
        "      # vocb_idf[word]: the word's idf\n",
        "      doc_tfidf_vec[document] = doc_tfidf_vec[document] + [(doc_vocab[document][word] / doc_vocab[document]['LENGTH']) * vocb_idf[word]]\n",
        "  return doc_tfidf_vec\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus, test_corpus_collection = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus_collection)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  #logging.debug(\"corpus: {}\".format(test_corpus))\n",
        "  #logging.debug(\"vocabpus: {}\".format(test_vocab))\n",
        "  #logging.debug(\"doc_vocab: {}\".format(test_doc_vocab))\n",
        "  #logging.debug(\"test_vocab_idf: {}\".format(test_vocab_idf))\n",
        "  \n",
        "  test_doc_tfidf_vec = cal_doc_tfidf_vec(test_vocab, test_doc_vocab, test_vocab_idf)\n",
        "  logging.debug(\"test_doc_tfidf_vec: {}\".format(test_doc_tfidf_vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Cwy_Cb-9cYwe"
      },
      "source": [
        "#Compute BoW (Bag-of-Words) Vector\n",
        "`doc_to_vec(doc, vocab)` returns a bag-of-words vector for document `doc`, corresponding to the presence of a word in `vocab`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l6X2fT2H8PAx"
      },
      "source": [
        "Compute the Bag-of-Words vector for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FLl103a8OLh",
        "colab": {}
      },
      "source": [
        "def doc2vec(doc):\n",
        "  words = tokenize(corpus_collection[doc])\n",
        "  return [1 if token in words else 0 for token, freq in vocab]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9q3bc4T_kzMy"
      },
      "source": [
        "Cosine similarity between two numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3WuWEBWLQBAM",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "  assert len(vec_a) == len(vec_b)\n",
        "  if sum(vec_a) == 0 or sum(vec_b) == 0:\n",
        "    return 0 # hack\n",
        "  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))\n",
        "  a_2 = sum([i*i for i in vec_a])\n",
        "  b_2 = sum([i*i for i in vec_b])\n",
        "  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FEyKg0mkeLm",
        "colab": {}
      },
      "source": [
        "# tfidf_vector = False\n",
        "tfidf_vector = True\n",
        "\n",
        "if tfidf_vector:\n",
        "  #\n",
        "  # calculate the similiarity based on word tfidf vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"tfidf {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc_tfidf_vec[doc_a], doc_tfidf_vec[doc_b])\n",
        "else:\n",
        "  #\n",
        "  # calculate the similiarity based on word bow vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"bow {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc2vec(doc_a), doc2vec(doc_b))\n",
        "  \n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  test_corpus, test_corpus_collection = get_corpus()\n",
        "  test_vocab, test_doc_vocab = get_vocab(test_corpus_collection)\n",
        "  test_vocab = test_vocab.most_common(100)\n",
        "  test_vocab_idf = cal_vocab_idf(test_vocab, test_doc_vocab)\n",
        "  doc_tfidf_vec = cal_doc_tfidf_vec(test_vocab, test_doc_vocab, test_vocab_idf)\n",
        "\n",
        "  seed_doc = test_corpus[5]\n",
        "  logging.debug('> \"{}\"'.format(seed_doc))\n",
        "  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(test_corpus)]\n",
        "  logging.debug(similarities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ddRwu-O1f13Q"
      },
      "source": [
        "# Find Similar Documents\n",
        "Find and print the $k$ most similar titles to a given title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a6rIkWUrmhxd",
        "colab": {}
      },
      "source": [
        "def k_similar(seed_id, k):\n",
        "  seed_title = corpus[seed_id]\n",
        "  print('> \"{}\"'.format(seed_title))\n",
        "  similarities = [doc_similarity(seed_title, title) for id, title in enumerate(corpus)]\n",
        "  logging.debug(\"Similiarities: {}\".format(similarities))\n",
        "  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
        "  nearest = [[corpus[id], similarities[id]] for id in top_indices]\n",
        "  print()\n",
        "  for story in reversed(nearest):\n",
        "    print('* \"{}\" ({})'.format(story[0], story[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IgPZe4rxUVPQ"
      },
      "source": [
        "# Test our program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjW8MQXZUmJU",
        "outputId": "3f05ee80-558c-4d0d-9c00-506d0c98c875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "corpus, corpus_collection = get_corpus()\n",
        "vocab, doc_vocab = get_vocab(corpus_collection)\n",
        "vocab = vocab.most_common(1000)\n",
        "vocab_idf = cal_vocab_idf(vocab, doc_vocab)\n",
        "doc_tfidf_vec = cal_doc_tfidf_vec(vocab, doc_vocab, vocab_idf)\n",
        "\n",
        "k_similar(686028 % 1000, 5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"U.S. banks’ dismal first quarter may spell trouble for 2016\"\n",
            "\n",
            "* \"U.S. banks’ dismal first quarter may spell trouble for 2016\" (1.0)\n",
            "* \"Citigroup’s first-quarter results suggest tough year ahead\" (0.5321500708114473)\n",
            "* \"Goldman posts weakest results in four years, revenue tumbles 40 percent\" (0.5198766373065498)\n",
            "* \"Investors look for trough in profit downturn\" (0.48185289938530335)\n",
            "* \"Wells Fargo searching for rainmaker in bid to boost dealmaking franchise\" (0.47481922359348333)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSC5PYzr_sSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}