{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hello Debug\n",
      "Hello Info\n",
      "Hello Warning\n",
      "Hello Error\n",
      "Hello Critical\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(message)s')\n",
    "\n",
    "logging.debug('Hello Debug')\n",
    "logging.info('Hello Info')\n",
    "logging.warning('Hello Warning')\n",
    "logging.error('Hello Error')\n",
    "logging.critical('Hello Critical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torchvision\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_letter = \",./;'[]<>?:\\\"\\{\\}!@#$%^&*()_+-=~`\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFiles(path):\n",
    "    return glob.glob(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Men Mon\n",
      "MnMon\n"
     ]
    }
   ],
   "source": [
    "def removeLetter(inputStr, removeLetter):\n",
    "    for i in removeLetter:\n",
    "        inputStr = inputStr.replace(i, \"\")\n",
    "    \n",
    "    return inputStr\n",
    "\n",
    "# unit test of removeLetter\n",
    "if __debug__:\n",
    "    oldstr = \"Men Mon\"\n",
    "    logging.debug(oldstr)\n",
    "    logging.debug(removeLetter(oldstr, \"e \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Men Mon\n",
      "men mon\n"
     ]
    }
   ],
   "source": [
    "def lowerCase(inputStr):\n",
    "    return inputStr.lower()\n",
    "\n",
    "# unit test of lowerCase\n",
    "if __debug__:\n",
    "    oldstr = \"Men Mon\"\n",
    "    logging.debug(oldstr)\n",
    "    logging.debug(lowerCase(oldstr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(inputStr):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', inputStr)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: ['his is a test.', 'Test .', 'Test ?', 'Test /', 'Test \"', 'Test :']\n",
      "lines: ['I am here.', 'I am not there.', 'WHere are you?', 'Where is she?']\n",
      "lines: ['This is a dog.', 'This is a cat.', 'Dog is not a cat.']\n",
      "all_lines: ['his is a test.', 'Test .', 'Test ?', 'Test /', 'Test \"', 'Test :', 'I am here.', 'I am not there.', 'WHere are you?', 'Where is she?', 'This is a dog.', 'This is a cat.', 'Dog is not a cat.']\n"
     ]
    }
   ],
   "source": [
    "def readLines(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    return [line for line in lines if line != '']\n",
    "    # return lines\n",
    "\n",
    "if __debug__:\n",
    "    all_lines = []\n",
    "    \n",
    "    for filename in findFiles('textFile/*.txt'):\n",
    "        lines = readLines(filename)\n",
    "        logging.debug('lines: {}'.format(lines))\n",
    "        all_lines = all_lines + lines\n",
    "    \n",
    "    logging.debug('all_lines: {}'.format(all_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test']\n",
      "words: ['i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she']\n",
      "words: ['this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n",
      "all_words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test', 'i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she', 'this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n"
     ]
    }
   ],
   "source": [
    "def readWords(filename):\n",
    "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        line = removeLetter(line, punctuation_letter)\n",
    "        if line != '':\n",
    "            words = words + lowerCase(line).strip().split(' ')\n",
    "    return words\n",
    "\n",
    "if __debug__:\n",
    "    all_words = []\n",
    "    \n",
    "    for filename in findFiles('textFile/*.txt'):\n",
    "        words = readWords(filename)\n",
    "        logging.debug('words: {}'.format(words))\n",
    "        all_words = all_words + words\n",
    "    \n",
    "    logging.debug('all_words: {}'.format(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test']\n",
      "words: ['i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she']\n",
      "words: ['this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n",
      "all_words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test', 'i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she', 'this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n",
      "unique_sorted_all_words: ['a' 'am' 'are' 'cat' 'dog' 'here' 'his' 'i' 'is' 'not' 'she' 'test'\n",
      " 'there' 'this' 'where' 'you']\n"
     ]
    }
   ],
   "source": [
    "def sortedUniqueWords(wordList):\n",
    "    return(np.unique(sorted(wordList)))\n",
    "    \n",
    "if __debug__:\n",
    "    all_words = []\n",
    "    \n",
    "    for filename in findFiles('textFile/*.txt'):\n",
    "        words = readWords(filename)\n",
    "        logging.debug('words: {}'.format(words))\n",
    "        all_words = all_words + words\n",
    "    \n",
    "    logging.debug('all_words: {}'.format((all_words)))\n",
    "    \n",
    "    unique_sorted_all_words = sortedUniqueWords(all_words)\n",
    "    logging.debug('unique_sorted_all_words: {}'.format(unique_sorted_all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file: textFile\\test 1.txt\n",
      "words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test']\n",
      "file: textFile\\test 2.txt\n",
      "words: ['i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she']\n",
      "file: textFile\\test 3.txt\n",
      "words: ['this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n",
      "\n",
      "words_in_a_document: {'test 1': ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test'], 'test 2': ['i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she'], 'test 3': ['this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']}\n",
      "\n",
      "all_words: ['his', 'is', 'a', 'test', 'test', 'test', 'test', 'test', 'test', 'i', 'am', 'here', 'i', 'am', 'not', 'there', 'where', 'are', 'you', 'where', 'is', 'she', 'this', 'is', 'a', 'dog', 'this', 'is', 'a', 'cat', 'dog', 'is', 'not', 'a', 'cat']\n",
      "sorted_unique_all_words: ['a' 'am' 'are' 'cat' 'dog' 'here' 'his' 'i' 'is' 'not' 'she' 'test'\n",
      " 'there' 'this' 'where' 'you']\n",
      "\n",
      "all_word_count\n",
      "OrderedDict([('test 1', OrderedDict([('LENGTH', 9), ('a', 1), ('am', 0), ('are', 0), ('cat', 0), ('dog', 0), ('here', 0), ('his', 1), ('i', 0), ('is', 1), ('not', 0), ('she', 0), ('test', 6), ('there', 0), ('this', 0), ('where', 0), ('you', 0)])), ('test 2', OrderedDict([('LENGTH', 13), ('a', 0), ('am', 2), ('are', 1), ('cat', 0), ('dog', 0), ('here', 1), ('his', 0), ('i', 2), ('is', 1), ('not', 1), ('she', 1), ('test', 0), ('there', 1), ('this', 0), ('where', 2), ('you', 1)])), ('test 3', OrderedDict([('LENGTH', 13), ('a', 3), ('am', 0), ('are', 0), ('cat', 2), ('dog', 2), ('here', 0), ('his', 0), ('i', 0), ('is', 3), ('not', 1), ('she', 0), ('test', 0), ('there', 0), ('this', 2), ('where', 0), ('you', 0)]))])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# words_in_a_document: collects of words. each collect corresponding to its own document\n",
    "# sorted_unique_all_words: sorted bag of words\n",
    "# allWordCount: two dimension of order dictionary. dimension 0 corrsponding to document.\n",
    "#               dimension 1 corresponding to word count in the document with <key, value> = <word, word count> pairs.\n",
    "#               a special key 'LENGTH' record total word counts\n",
    "#\n",
    "\n",
    "all_words = []\n",
    "words_in_a_document = {}\n",
    "    \n",
    "for filename in findFiles('textFile/*.txt'):\n",
    "    words = readWords(filename)\n",
    "    logging.debug('file: {}'.format(filename))\n",
    "    logging.debug('words: {}'.format(words))\n",
    "    words_in_a_document[os.path.splitext(os.path.basename(filename))[0]] = words\n",
    "    all_words = all_words + words\n",
    "    \n",
    "logging.debug('\\nwords_in_a_document: {}'.format((words_in_a_document)))\n",
    "\n",
    "logging.debug('\\nall_words: {}'.format((all_words)))    \n",
    "sorted_unique_all_words = sortedUniqueWords(all_words)\n",
    "logging.debug('sorted_unique_all_words: {}'.format(sorted_unique_all_words))\n",
    "    \n",
    "all_word_count = OrderedDict()\n",
    "    \n",
    "for document in words_in_a_document:\n",
    "    words = words_in_a_document[document]\n",
    "    wordCount = OrderedDict()\n",
    "    wordCount['LENGTH'] = len(words)\n",
    "    for w in sorted_unique_all_words:\n",
    "        # logging.debug(\"{}: {} times\".format(w, words.count(w)))\n",
    "        wordCount[w] = words.count(w)\n",
    "    all_word_count[document] = wordCount\n",
    "\n",
    "logging.debug(\"\\nall_word_count\")\n",
    "logging.debug(all_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "number of bag of words: 16\n",
      "\n",
      "document test 1\n",
      "OrderedDict([('a', 0.08), ('am', 0.04), ('are', 0.04), ('cat', 0.04), ('dog', 0.04), ('here', 0.04), ('his', 0.08), ('i', 0.04), ('is', 0.08), ('not', 0.04), ('she', 0.04), ('test', 0.28), ('there', 0.04), ('this', 0.04), ('where', 0.04), ('you', 0.04)])\n",
      "\n",
      "document test 2\n",
      "OrderedDict([('a', 0.034482758620689655), ('am', 0.10344827586206896), ('are', 0.06896551724137931), ('cat', 0.034482758620689655), ('dog', 0.034482758620689655), ('here', 0.06896551724137931), ('his', 0.034482758620689655), ('i', 0.10344827586206896), ('is', 0.06896551724137931), ('not', 0.06896551724137931), ('she', 0.06896551724137931), ('test', 0.034482758620689655), ('there', 0.06896551724137931), ('this', 0.034482758620689655), ('where', 0.10344827586206896), ('you', 0.06896551724137931)])\n",
      "\n",
      "document test 3\n",
      "OrderedDict([('a', 0.13793103448275862), ('am', 0.034482758620689655), ('are', 0.034482758620689655), ('cat', 0.10344827586206896), ('dog', 0.10344827586206896), ('here', 0.034482758620689655), ('his', 0.034482758620689655), ('i', 0.034482758620689655), ('is', 0.13793103448275862), ('not', 0.06896551724137931), ('she', 0.034482758620689655), ('test', 0.034482758620689655), ('there', 0.034482758620689655), ('this', 0.10344827586206896), ('where', 0.034482758620689655), ('you', 0.034482758620689655)])\n",
      "\n",
      "words tf\n",
      "OrderedDict([('test 1', OrderedDict([('a', 0.08), ('am', 0.04), ('are', 0.04), ('cat', 0.04), ('dog', 0.04), ('here', 0.04), ('his', 0.08), ('i', 0.04), ('is', 0.08), ('not', 0.04), ('she', 0.04), ('test', 0.28), ('there', 0.04), ('this', 0.04), ('where', 0.04), ('you', 0.04)])), ('test 2', OrderedDict([('a', 0.034482758620689655), ('am', 0.10344827586206896), ('are', 0.06896551724137931), ('cat', 0.034482758620689655), ('dog', 0.034482758620689655), ('here', 0.06896551724137931), ('his', 0.034482758620689655), ('i', 0.10344827586206896), ('is', 0.06896551724137931), ('not', 0.06896551724137931), ('she', 0.06896551724137931), ('test', 0.034482758620689655), ('there', 0.06896551724137931), ('this', 0.034482758620689655), ('where', 0.10344827586206896), ('you', 0.06896551724137931)])), ('test 3', OrderedDict([('a', 0.13793103448275862), ('am', 0.034482758620689655), ('are', 0.034482758620689655), ('cat', 0.10344827586206896), ('dog', 0.10344827586206896), ('here', 0.034482758620689655), ('his', 0.034482758620689655), ('i', 0.034482758620689655), ('is', 0.13793103448275862), ('not', 0.06896551724137931), ('she', 0.034482758620689655), ('test', 0.034482758620689655), ('there', 0.034482758620689655), ('this', 0.10344827586206896), ('where', 0.034482758620689655), ('you', 0.034482758620689655)]))])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# count words tf, words_tf\n",
    "#\n",
    "\n",
    "words_tf = OrderedDict()\n",
    "num_bow = len(sorted_unique_all_words)\n",
    "logging.debug(\"number of bag of words: {}\\n\".format(num_bow))\n",
    "\n",
    "for document in all_word_count:\n",
    "    tf = OrderedDict()\n",
    "    for word in all_word_count[document]:\n",
    "        if word != 'LENGTH':\n",
    "            tf[word] = (all_word_count[document][word] + 1) / (all_word_count[document]['LENGTH'] + num_bow)\n",
    "    logging.debug(\"document {}\".format(document))\n",
    "    logging.debug(\"{}\\n\".format(tf))\n",
    "    words_tf[document] = tf\n",
    "logging.debug(\"words tf\")\n",
    "logging.debug(\"{}\\n\".format(words_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "word a's document count = 3\n",
      "word am's document count = 2\n",
      "word are's document count = 2\n",
      "word cat's document count = 2\n",
      "word dog's document count = 2\n",
      "word here's document count = 2\n",
      "word his's document count = 2\n",
      "word i's document count = 2\n",
      "word is's document count = 4\n",
      "word not's document count = 3\n",
      "word she's document count = 2\n",
      "word test's document count = 2\n",
      "word there's document count = 2\n",
      "word this's document count = 2\n",
      "word where's document count = 2\n",
      "word you's document count = 2\n",
      "\n",
      "words idf\n",
      "OrderedDict([('a', 1.3333333333333333), ('am', 2.0), ('are', 2.0), ('cat', 2.0), ('dog', 2.0), ('here', 2.0), ('his', 2.0), ('i', 2.0), ('is', 1.0), ('not', 1.3333333333333333), ('she', 2.0), ('test', 2.0), ('there', 2.0), ('this', 2.0), ('where', 2.0), ('you', 2.0)])\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# count words idf, words_idf\n",
    "#\n",
    "\n",
    "words_idf = OrderedDict()\n",
    "documents = len(all_word_count)\n",
    "\n",
    "for word in sorted_unique_all_words:\n",
    "    doc_count = 1\n",
    "    for document in all_word_count:\n",
    "        if all_word_count[document][word] != 0:\n",
    "            doc_count = doc_count + 1\n",
    "    logging.debug(\"word {}\\'s document count = {}\".format(word, doc_count))\n",
    "    words_idf[word] = (documents + 1) / doc_count\n",
    "logging.debug(\"\\nwords idf\")\n",
    "logging.debug(words_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "document test 1\n",
      "OrderedDict([('a', -0.43892477839364497), ('am', -1.5380075220200162), ('are', -1.5380075220200162), ('cat', -1.5380075220200162), ('dog', -1.5380075220200162), ('here', -1.5380075220200162), ('his', -1.057554508101815), ('i', -1.5380075220200162), ('is', -0.0), ('not', -0.6383307958112386), ('she', -1.5380075220200162), ('test', -0.1892053885793429), ('there', -1.5380075220200162), ('this', -1.5380075220200162), ('where', -1.5380075220200162), ('you', -1.5380075220200162)])\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "16\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-5f59ef7b9064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"document {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mwords_tfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mdocument_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocument_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "#\n",
    "# count words tfidf, words_tfidf in python dictionary implementation\n",
    "# also tfidf_array in python numpy array implementation\n",
    "#\n",
    "\n",
    "words_tfidf = OrderedDict()\n",
    "print(len(all_word_count))\n",
    "print(len(sorted_unique_all_words))\n",
    "tfidf_array = np.zeros([len(all_word_count), len(sorted_unique_all_words)])\n",
    "print(tfidf_array)\n",
    "\n",
    "document_index = 0\n",
    "for document in all_word_count:\n",
    "    tfidf = OrderedDict()\n",
    "    word_index = 0\n",
    "    for word in all_word_count[document]:\n",
    "        if word != 'LENGTH':\n",
    "            # logging.debug(\"tf: {:.6f}, idf: {:.6f}, log of tf: {:.6f}, log of idf: {:.6f}, (1+logtf)*logidf: {:.6f}\".format(\n",
    "            #    words_tf[document][word], words_idf[word],\n",
    "            #    math.log(words_tf[document][word]), math.log(words_idf[word]),\n",
    "            #    (1 + math.log(words_tf[document][word])) * math.log(words_idf[word])))\n",
    "            tfidf[word] = (1 + math.log(words_tf[document][word])) * math.log(words_idf[word])\n",
    "            tfidf_array[document_index][word_index] = tfidf[word]\n",
    "            word_index = word_index + 1\n",
    "            \n",
    "    logging.debug(\"document {}\".format(document))\n",
    "    logging.debug(\"{}\\n\".format(tfidf))\n",
    "    words_tfidf[document] = tfidf\n",
    "    document_index = document_index + 1\n",
    "    \n",
    "logging.debug(\"words tfidf\")\n",
    "logging.debug(\"{}\\n\".format(words_tfidf))\n",
    "logging.debug(\"{}\\n\".format(tfidf_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    test 1              test 2              test 3              \n",
      "a                -0.4389247784       -0.6810285705       -0.2822165356\n",
      "am               -1.5380075220       -0.8793844197       -1.6408844301\n",
      "are              -1.5380075220       -1.1604314162       -1.6408844301\n",
      "cat              -1.5380075220       -1.6408844301       -0.8793844197\n",
      "dog              -1.5380075220       -1.6408844301       -0.8793844197\n",
      "here             -1.5380075220       -1.1604314162       -1.6408844301\n",
      "his              -1.0575545081       -1.6408844301       -1.6408844301\n",
      "i                -1.5380075220       -0.8793844197       -1.6408844301\n",
      "is               -0.0000000000       -0.0000000000       -0.0000000000\n",
      "not              -0.6383307958       -0.4816225531       -0.4816225531\n",
      "she              -1.5380075220       -1.1604314162       -1.6408844301\n",
      "test             -0.1892053886       -1.6408844301       -1.6408844301\n",
      "there            -1.5380075220       -1.1604314162       -1.6408844301\n",
      "this             -1.5380075220       -1.6408844301       -0.8793844197\n",
      "where            -1.5380075220       -0.8793844197       -1.6408844301\n",
      "you              -1.5380075220       -1.1604314162       -1.6408844301\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# print out word's tf.idf\n",
    "#\n",
    "\n",
    "print(\"{:20s}\".format(\"\"), end='')\n",
    "for document in words_tfidf:\n",
    "    print(\"{:20s}\".format(document), end='')\n",
    "print(\"\")\n",
    "for word in sorted_unique_all_words:\n",
    "    print(\"{:10s}\".format(word), end='')\n",
    "    for document in words_tfidf:\n",
    "        print(\"{:20.10f}\".format(words_tfidf[document][word]), end='')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "['is' 'test' 'a']\n",
      "['a' 'am' 'are']\n",
      "['a' 'am' 'are']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# show top k tfidf words in document\n",
    "#\n",
    "\n",
    "#print(OrderedDict(sorted(k.items(), key=lambda t: t[1])))\n",
    "\n",
    "top_k = 3\n",
    "for i in range(np.shape(tfidf_array)[0]):\n",
    "    # logging.debug(heapq.nlargest(3, range(len(tfidf_array[i])), tfidf_array[i].__getitem__))\n",
    "    # print(words_tfidf[i])\n",
    "    logging.debug(sorted_unique_all_words[heapq.nlargest(top_k, range(len(tfidf_array[i])), tfidf_array[i].__getitem__)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
