{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2-0686028.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrVmtCF/CCyzkm56BNJqsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chilung/EmotionX2020/blob/master/lab2_0686028.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "op_4u74kKNRO",
        "colab_type": "code",
        "outputId": "d4f3dbff-8113-4b57-b946-bc7f976f2aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
        "\n",
        "logging.debug('Hello Debug')\n",
        "logging.info('Hello Info')\n",
        "logging.warning('Hello Warning')\n",
        "logging.error('Hello Error')\n",
        "logging.critical('Hello Critical')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello Info\n",
            "Hello Warning\n",
            "Hello Error\n",
            "Hello Critical\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVHwZKOZukz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk import pos_tag\n",
        "import re\n",
        "import pandas as pd\n",
        "from io import open\n",
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import OrderedDict\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torchvision\n",
        "import heapq\n",
        "import unicodedata\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s3kMqY7So_n",
        "colab_type": "text"
      },
      "source": [
        "# Hands-On Lab2 Descriptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3KTzUg-H-hS",
        "colab_type": "text"
      },
      "source": [
        "1. Calculate and print the \n",
        "* 5 most frequent 2-grams \n",
        "* from the Reuters news dataset (content) available at bit.ly/nlp-reuters\n",
        "* where both tokens are PROPER NOUNS\n",
        "* using NLTK word_tokenize, POS tagger\n",
        "* No need to remove punctuation, no need to remove stop words\n",
        "2. Calculate and print the\n",
        "* 5 most similar articles to seed_id = <student_ID> % 1000\n",
        "* from the Buzzfeed new dataset (content), available at bit.ly/nlp-buzzfeed\n",
        "* Tokens are lemma + POS (e.g., “give_VERB”)\n",
        "* using the SpaCy POS tagger and tokenizer\n",
        "* Use en_core_web_sm model\n",
        "* Remove stopwords\n",
        "* TF-IDF with 512 features (most common tokens)\n",
        "* Can use TF-IDF code from here: Medium article\n",
        "* Can use library function to compute cosine_similarity\n",
        "3. Use the same Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7--qE6JSofO",
        "colab_type": "text"
      },
      "source": [
        "#Task 1: Subroutines\n",
        "Calculate and print the\n",
        "* 5 most frequent 2-grams\n",
        "* from the Reuters news dataset (content) available at bit.ly/nlp-reuters\n",
        "* where both tokens are PROPER NOUNS\n",
        "* using NLTK word_tokenize, POS tagger\n",
        "* No need to remove punctuation, no need to remove stop words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d8MZkiGupMx",
        "colab_type": "code",
        "outputId": "4e6eded5-f2aa-4eb3-90ff-a204dcc8df3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "# stopwords.words('english')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKzDWi6YIx6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_corpus(corpus_path, req_size, to_lower = True, title_only=False):\n",
        "  df = pd.read_csv(corpus_path)\n",
        "  logging.debug(\"Dataset columns: {}\".format(df.columns))\n",
        "  logging.debug(\"Dataset size: {}\".format(len(df)))\n",
        "\n",
        "  if req_size == 'ALL':\n",
        "    req_size = len(df)\n",
        "  num_of_documents = req_size if req_size < len(df) else len(df)\n",
        "  \n",
        "  corpus_title = df.title.to_list()[0:num_of_documents]\n",
        "  corpus_content = df.content.to_list()[0:num_of_documents]\n",
        "\n",
        "  corpus = {}\n",
        "  for index in range(len(corpus_title)):\n",
        "    if pd.isnull(corpus_title[index]):\n",
        "      corpus_title[index] = \"NULL\"\n",
        "    if pd.isnull(corpus_content[index]):\n",
        "      corpus_content[index] = \"NULL\"\n",
        "\n",
        "    if to_lower == True:\n",
        "      corpus_title[index] = corpus_title[index].lower()\n",
        "      corpus_content[index] = corpus_content[index].lower()\n",
        "\n",
        "    if title_only == True:\n",
        "      corpus[corpus_title[index]] = corpus_title[index]\n",
        "    else:\n",
        "      corpus[corpus_title[index]] = corpus_content[index]\n",
        "\n",
        "  return corpus_title, corpus\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv', 100, to_lower=False, title_only=False)\n",
        "  logging.debug(corpus_title[0:50])\n",
        "  for index, title in enumerate(corpus):\n",
        "    if index < 50:\n",
        "      logging.debug(\"index: {}, content:{}\".format(index, corpus[title]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQizWpFlXaea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sep_words = [('U. S.', 'U.S.'), ('U. N.', 'U.N.')]\n",
        "\n",
        "def sep_word_preprocessing(doc):\n",
        "  for sep_word in sep_words:\n",
        "    sep, merge = sep_word\n",
        "    doc = doc.replace(sep, merge)\n",
        "  return doc\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  logging.debug(sep_word_preprocessing(corpus['Exclusive: Apple makes iPhone screen fixes easier as states mull repair laws']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-T9U6jVxU3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# use library word_tokenize\n",
        "#\n",
        "def get_tokens_nltk(corpus):\n",
        "  tokens = []\n",
        "  for index, title in enumerate(corpus):\n",
        "    if (index % 1000) == 0:\n",
        "      logging.info(\"word_tokenize document {}\".format(index))\n",
        "    #doc = title\n",
        "    doc = corpus[title]\n",
        "    doc = sep_word_preprocessing(doc)\n",
        "    tokens = tokens + [token for token in word_tokenize(doc)]\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  tokens = get_tokens_nltk(corpus)\n",
        "  logging.debug(tokens[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9NDm_oyBWtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# use stopwords\n",
        "#\n",
        "def remove_stopwords(tokens):\n",
        "  logging.info(\"Total Tokens: {}\".format(len(tokens)))\n",
        "\n",
        "  step = 100000\n",
        "  new_tokens = []\n",
        "\n",
        "  for i in range(int(len(tokens)/step)):\n",
        "    logging.info(\"Process removing stop words: {} to {}\".format(i*step, (i+1)*step))\n",
        "    new_tokens = new_tokens + [token for token in tokens[i*step:(i+1)*step] if token not in stopwords.words('english')]\n",
        "\n",
        "  i = int(len(tokens)/step)\n",
        "  j = len(tokens) % step\n",
        "  logging.info(\"Process removing stop words: {} to {}\".format(i*step, i*step+j))\n",
        "  new_tokens = new_tokens + [token for token in tokens[i*step:i*step+j] if token not in stopwords.words('english')]\n",
        "\n",
        "  return new_tokens\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  tokens = remove_stopwords(tokens)\n",
        "  logging.debug(tokens[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embBudtz2FTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngram(tokens, n):\n",
        "  return list(ngrams(tokens, n))\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  two_grams = get_ngram(tokens, 2)\n",
        "  logging.debug(two_grams[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no5Lr2KXEfSe",
        "colab_type": "code",
        "outputId": "58b9a353-8dd7-4de0-900f-22eee2f05f20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#\n",
        "# use nltk averaged_perceptron_tagger Part of Speech\n",
        "#\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def to_token_pos(tokens):\n",
        "  token_pos_pairs = pos_tag(tokens)\n",
        "  return token_pos_pairs\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  logging.debug(to_token_pos(tokens)[0:100])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZAESkuOMvo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_proper_noun_pairs(two_grams):\n",
        "  nnp_word_pairs = []\n",
        "  nnp_word_pos_pairs = []\n",
        "\n",
        "  logging.info(\"Total Two-Gram Pairs: {}\".format(len(two_grams)))\n",
        "  for index, pair in enumerate(two_grams):\n",
        "    if (index % 100000) == 0:\n",
        "      logging.info(\"Checking Proper Noun Pairs. Progress: {}\".format(index))\n",
        "    (word1, pos1), (word2, pos2) = to_token_pos(pair)\n",
        "    #logging.debug(\"{}, {}, {}, {}\".format(word1, pos1, word2, pos2))\n",
        "    if pos1 == 'NNP' and pos2 == 'NNP':\n",
        "      nnp_word_pairs = nnp_word_pairs + [(word1, word2)]\n",
        "      nnp_word_pos_pairs = nnp_word_pos_pairs + [(word1+'_'+pos1, word2+'_'+pos2)]\n",
        "\n",
        "  return nnp_word_pairs, nnp_word_pos_pairs\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  proper_noun_pairs, proper_noun_pos_pairs = find_proper_noun_pairs(two_grams)\n",
        "  logging.debug(proper_noun_pairs[0:100])\n",
        "  logging.debug(proper_noun_pos_pairs[0:100])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0u-wdMvaPqn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nnp_pairs_counter(proper_noun_pairs):\n",
        "  nnp_pairs_collection = Counter()\n",
        "  logging.info(\"Total Proper Noun Pairs: {}\".format(len(proper_noun_pairs)))\n",
        "  for index, nnp_pair in enumerate(proper_noun_pairs):\n",
        "    if (index % 100000) == 0:\n",
        "      logging.info(\"Counter Collection, Progress: {}\".format(index))\n",
        "    nnp_pairs_collection.update([nnp_pair])\n",
        "\n",
        "  return nnp_pairs_collection\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  nnp_pairs_collection = nnp_pairs_counter(proper_noun_pairs)\n",
        "  logging.debug(nnp_pairs_collection)\n",
        "  logging.debug(nnp_pairs_collection.most_common(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjsaFBYxbnNF",
        "colab_type": "text"
      },
      "source": [
        "# Task 1: Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AjvtWjKa7Qj",
        "colab_type": "code",
        "outputId": "08dfea5e-5f89-42a6-ff23-e52da5c782c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/reuters.csv', 'ALL', to_lower=False, title_only=False)\n",
        "logging.debug(\"corpus document number: {}\".format(len(corpus)))\n",
        "#logging.debug(corpus)\n",
        "\n",
        "tokens = get_tokens_nltk(corpus)\n",
        "logging.debug(tokens)\n",
        "\n",
        "tokens = remove_stopwords(tokens)\n",
        "logging.debug(tokens)\n",
        "\n",
        "two_grams = get_ngram(tokens, 2)\n",
        "logging.debug(two_grams)\n",
        "\n",
        "proper_noun_pairs, proper_noun_pos_pairs = find_proper_noun_pairs(two_grams)\n",
        "logging.debug(proper_noun_pairs)\n",
        "logging.debug(proper_noun_pos_pairs)\n",
        "\n",
        "nnp_pairs_collection = nnp_pairs_counter(proper_noun_pairs)\n",
        "logging.debug(nnp_pairs_collection)\n",
        "logging.info(nnp_pairs_collection.most_common(5))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word_tokenize document 0\n",
            "word_tokenize document 1000\n",
            "word_tokenize document 2000\n",
            "word_tokenize document 3000\n",
            "word_tokenize document 4000\n",
            "word_tokenize document 5000\n",
            "Total Tokens: 4094546\n",
            "Process removing stop words: 0 to 100000\n",
            "Process removing stop words: 100000 to 200000\n",
            "Process removing stop words: 200000 to 300000\n",
            "Process removing stop words: 300000 to 400000\n",
            "Process removing stop words: 400000 to 500000\n",
            "Process removing stop words: 500000 to 600000\n",
            "Process removing stop words: 600000 to 700000\n",
            "Process removing stop words: 700000 to 800000\n",
            "Process removing stop words: 800000 to 900000\n",
            "Process removing stop words: 900000 to 1000000\n",
            "Process removing stop words: 1000000 to 1100000\n",
            "Process removing stop words: 1100000 to 1200000\n",
            "Process removing stop words: 1200000 to 1300000\n",
            "Process removing stop words: 1300000 to 1400000\n",
            "Process removing stop words: 1400000 to 1500000\n",
            "Process removing stop words: 1500000 to 1600000\n",
            "Process removing stop words: 1600000 to 1700000\n",
            "Process removing stop words: 1700000 to 1800000\n",
            "Process removing stop words: 1800000 to 1900000\n",
            "Process removing stop words: 1900000 to 2000000\n",
            "Process removing stop words: 2000000 to 2100000\n",
            "Process removing stop words: 2100000 to 2200000\n",
            "Process removing stop words: 2200000 to 2300000\n",
            "Process removing stop words: 2300000 to 2400000\n",
            "Process removing stop words: 2400000 to 2500000\n",
            "Process removing stop words: 2500000 to 2600000\n",
            "Process removing stop words: 2600000 to 2700000\n",
            "Process removing stop words: 2700000 to 2800000\n",
            "Process removing stop words: 2800000 to 2900000\n",
            "Process removing stop words: 2900000 to 3000000\n",
            "Process removing stop words: 3000000 to 3100000\n",
            "Process removing stop words: 3100000 to 3200000\n",
            "Process removing stop words: 3200000 to 3300000\n",
            "Process removing stop words: 3300000 to 3400000\n",
            "Process removing stop words: 3400000 to 3500000\n",
            "Process removing stop words: 3500000 to 3600000\n",
            "Process removing stop words: 3600000 to 3700000\n",
            "Process removing stop words: 3700000 to 3800000\n",
            "Process removing stop words: 3800000 to 3900000\n",
            "Process removing stop words: 3900000 to 4000000\n",
            "Process removing stop words: 4000000 to 4094546\n",
            "Total Two-Gram Pairs: 2768811\n",
            "Checking Proper Noun Pairs. Progress: 0\n",
            "Checking Proper Noun Pairs. Progress: 100000\n",
            "Checking Proper Noun Pairs. Progress: 200000\n",
            "Checking Proper Noun Pairs. Progress: 300000\n",
            "Checking Proper Noun Pairs. Progress: 400000\n",
            "Checking Proper Noun Pairs. Progress: 500000\n",
            "Checking Proper Noun Pairs. Progress: 600000\n",
            "Checking Proper Noun Pairs. Progress: 700000\n",
            "Checking Proper Noun Pairs. Progress: 800000\n",
            "Checking Proper Noun Pairs. Progress: 900000\n",
            "Checking Proper Noun Pairs. Progress: 1000000\n",
            "Checking Proper Noun Pairs. Progress: 1100000\n",
            "Checking Proper Noun Pairs. Progress: 1200000\n",
            "Checking Proper Noun Pairs. Progress: 1300000\n",
            "Checking Proper Noun Pairs. Progress: 1400000\n",
            "Checking Proper Noun Pairs. Progress: 1500000\n",
            "Checking Proper Noun Pairs. Progress: 1600000\n",
            "Checking Proper Noun Pairs. Progress: 1700000\n",
            "Checking Proper Noun Pairs. Progress: 1800000\n",
            "Checking Proper Noun Pairs. Progress: 1900000\n",
            "Checking Proper Noun Pairs. Progress: 2000000\n",
            "Checking Proper Noun Pairs. Progress: 2100000\n",
            "Checking Proper Noun Pairs. Progress: 2200000\n",
            "Checking Proper Noun Pairs. Progress: 2300000\n",
            "Checking Proper Noun Pairs. Progress: 2400000\n",
            "Checking Proper Noun Pairs. Progress: 2500000\n",
            "Checking Proper Noun Pairs. Progress: 2600000\n",
            "Checking Proper Noun Pairs. Progress: 2700000\n",
            "Total Proper Noun Pairs: 164817\n",
            "Counter Collection, Progress: 0\n",
            "Counter Collection, Progress: 100000\n",
            "[(('Donald', 'Trump'), 3212), (('New', 'York'), 2442), (('Islamic', 'State'), 1952), (('President', 'Donald'), 1922), (('North', 'Korea'), 1650)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2g3o1uspgA3",
        "colab_type": "text"
      },
      "source": [
        "# Task 1: Output Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1StouUypd0f",
        "colab_type": "code",
        "outputId": "14ae1fa7-7a76-4604-8a3a-f94193f309b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "logging.info(nnp_pairs_collection.most_common(5))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('Donald', 'Trump'), 3212), (('New', 'York'), 2442), (('Islamic', 'State'), 1952), (('President', 'Donald'), 1922), (('North', 'Korea'), 1650)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qq6VXY-peBn8",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Subroutines\n",
        "Calculate and print the\n",
        "* 5 most similar articles to seed_id = % 1000\n",
        "* from the Buzzfeed new dataset (content), available at bit.ly/nlp-buzzfeed\n",
        "* Tokens are lemma + POS (e.g., “give_VERB”)\n",
        "* using the SpaCy POS tagger and tokenizer\n",
        "* Use en_core_web_sm model\n",
        "* Remove stopwords\n",
        "* TF-IDF with 512 features (most common tokens)\n",
        "* Can use TF-IDF code from here: Medium article\n",
        "* Can use library function to compute cosine_similarity\n",
        "* Use the same Notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI34F1ie3Ffb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#\n",
        "# use library spacy nlp\n",
        "#\n",
        "def get_tokens_spacy(document):\n",
        "  tokens = [token for token in nlp(document)]\n",
        "  return tokens\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv', 10, to_lower=False, title_only=True)\n",
        "  logging.debug(\"corpus document number: {}\".format(len(corpus)))\n",
        "  logging.debug(\"Corpus:\\n{}\".format(corpus))\n",
        "  \n",
        "  tokens = []\n",
        "  for title in corpus:\n",
        "    tokens = tokens + get_tokens_spacy(corpus[title])\n",
        "  logging.debug(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrIUDPA58MVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# use stopwords from spacy tokens\n",
        "#\n",
        "def remove_stop_punct_space(tokens):\n",
        "  return [token for token in tokens if not (token.is_stop or token.is_punct or token.is_space)]\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  tokens = remove_stop_punct_space(tokens)\n",
        "  logging.debug(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Nyh2icB7JTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# change token to lemma+pos\n",
        "#\n",
        "def token_to_lemma_pos(tokens):\n",
        "  return [token.lemma_+'_'+token.pos_ for token in tokens]\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  tokens = token_to_lemma_pos(tokens)\n",
        "  logging.debug(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRDUSjk-Hv0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(document, verbose=False):\n",
        "  tokens = get_tokens_spacy(document)\n",
        "  if verbose == True:\n",
        "    logging.debug(\"NLP tokens:\\n{}\".format(tokens))\n",
        "  \n",
        "  tokens = remove_stop_punct_space(tokens)\n",
        "  if verbose == True:\n",
        "    logging.debug(\"Remove Stopwords:\\n{}\".format(tokens))\n",
        "\n",
        "  tokens = token_to_lemma_pos(tokens)\n",
        "  if verbose == True:\n",
        "    logging.debug(\"Lemma+POS:\\n{}\".format(tokens))\n",
        "\n",
        "  return tokens\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv', 100, to_lower=False, title_only=False)\n",
        "  logging.debug(\"corpus document number: {}\".format(len(corpus)))\n",
        "  logging.debug(\"Corpus:\\n{}\".format(corpus))\n",
        "  \n",
        "  for index, title in enumerate(corpus):\n",
        "    if index < 10:\n",
        "      tokens = tokenize(corpus[title], verbose=True)\n",
        "      logging.debug(tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2OU9h2IhHd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_counter(tokens):\n",
        "  tokens_collection = Counter()\n",
        "  for token in tokens:\n",
        "    tokens_collection.update([token])\n",
        "\n",
        "  return tokens_collection\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  tokens_collection = tokens_counter(tokens)\n",
        "  logging.debug(tokens_collection)\n",
        "  logging.debug(tokens_collection.most_common(5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lOd15N5wmX8",
        "colab_type": "code",
        "outputId": "0ec6b692-ab9b-4f31-bf11-9e9f09eba5d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(\"Taiwan gives 400,000 masks to U.S. under cooperation arrangement. #TaiwanCanHelp :)\"))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Taiwan', 'gives', '400,000', 'masks', 'to', 'U', '.', 'S', '.', 'under', 'cooperation', 'arrangement', '.', '#TaiwanCanHelp', ':)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TknJb-kYyTSf",
        "colab_type": "code",
        "outputId": "a2488fbc-792e-4486-893b-ab342d63f68d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "print(sent_tokenize(\"Hello! and welcome to U.S.! How are you? I feel so happy... and you?\"))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello!', 'and welcome to U.S.!', 'How are you?', 'I feel so happy... and you?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdcSIEE_ASYn",
        "colab_type": "text"
      },
      "source": [
        "#Computing word frequencies\n",
        "`get_vocab(corpus)` computes the word frequencies in a given corpus. It also collect word frequencies of individual document in the given corpus. It returns two items. The first item is a list of 2-tuples, `vocab`: each tuple contains the token and its frequency. The second item is two dimension list, `doc_vocab`: a list of individual document's list of 2-tuples, each tuple contains the token and its frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYopd-XkAKV-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(corpus_collection, verbose=False):\n",
        "  vocabulary = Counter()\n",
        "  doc_vocab = {}\n",
        "  for title in corpus_collection:\n",
        "\n",
        "    if verbose == True:\n",
        "      logging.debug(\"origin document: {}\".format(corpus_collection[title]))\n",
        "    tokens = tokenize(corpus_collection[title])\n",
        "    if verbose == True:\n",
        "      logging.debug(\"token: {}\".format(tokens))\n",
        "\n",
        "    doc_vocab_collection = Counter()\n",
        "    doc_vocab_collection.update(tokens)\n",
        "    doc_vocab_collection['LENGTH'] = len(tokens)\n",
        "    \n",
        "    doc_vocab[title] = doc_vocab_collection\n",
        "    vocabulary.update(tokens)\n",
        "  \n",
        "  return vocabulary, doc_vocab\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv', 20, to_lower=False, title_only=False)\n",
        "  logging.debug(\"corpus document number: {}\".format(len(corpus)))\n",
        "  logging.debug(\"Corpus:\\n{}\".format(corpus))\n",
        "  \n",
        "  vocab, doc_vocab = get_vocab(corpus, verbose=True)\n",
        "  logging.debug(\"vocab: {}\".format(vocab))\n",
        "  logging.debug(\"doc_vocab: {}\".format(doc_vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "us8g8hsBAaLN",
        "colab_type": "text"
      },
      "source": [
        "#Compute Words' IDF and TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yuk2lRz0AXYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "# 'cal_vocab_idf' calculates the idf of each word.\n",
        "# The idf of word is given by log of the result of total number of documents divided by the number of documents in which the word happens.\n",
        "#\n",
        "\n",
        "def cal_vocab_idf(vocab, doc_vocab):\n",
        "  vocab_idf = Counter()\n",
        "  for i, token in enumerate(vocab):\n",
        "    word, freq = token\n",
        "    vocab_idf[word] = 0\n",
        "    for document in doc_vocab:\n",
        "      if word in doc_vocab[document]:\n",
        "        vocab_idf[word] = vocab_idf[word] + 1\n",
        "    vocab_idf[word] = math.log(len(doc_vocab) / vocab_idf[word])\n",
        "  return vocab_idf\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  vocab = vocab.most_common(100)\n",
        "  vocab_idf = cal_vocab_idf(vocab, doc_vocab)\n",
        "  logging.debug(\"test_vocab_idf: {}\".format(vocab_idf))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKnr3NICAdVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cal_doc_tfidf_vec(vocab, doc_vocab, vocb_idf):\n",
        "  doc_tfidf_vec = {}\n",
        "  for document in doc_vocab:\n",
        "    doc_tfidf_vec[document] = []\n",
        "    for i, token in enumerate(vocab): \n",
        "      word, freq = token\n",
        "      # doc_vocab[document][word]: the count of the word in this document\n",
        "      # doc_vocab[document]['LENGTH']: the count of total words in this document\n",
        "      # vocb_idf[word]: the word's idf\n",
        "      doc_tfidf_vec[document] = doc_tfidf_vec[document] + [(doc_vocab[document][word] / doc_vocab[document]['LENGTH']) * vocb_idf[word]]\n",
        "  return doc_tfidf_vec\n",
        "\n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  doc_tfidf_vec = cal_doc_tfidf_vec(vocab, doc_vocab, vocab_idf)\n",
        "  logging.debug(\"test_doc_tfidf_vec: {}\".format(doc_tfidf_vec))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1osmZk0RAjSC",
        "colab_type": "text"
      },
      "source": [
        "Compute BoW (Bag-of-Words) Vector\n",
        "doc_to_vec(doc, vocab) returns a bag-of-words vector for document doc, corresponding to the presence of a word in vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cm_PMCpaAmni",
        "colab_type": "text"
      },
      "source": [
        "Compute the Bag-of-Words vector for each document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga6tUsntAomY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doc2vec(doc):\n",
        "  words = tokenize(corpus_collection[doc])\n",
        "  return [1 if token in words else 0 for token, freq in vocab]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdCpyVL7Aqvs",
        "colab_type": "text"
      },
      "source": [
        "Cosine similarity between two numerical vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgq8hryYAsyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cosine_similarity(vec_a, vec_b):\n",
        "  assert len(vec_a) == len(vec_b)\n",
        "  if sum(vec_a) == 0 or sum(vec_b) == 0:\n",
        "    return 0 # hack\n",
        "  a_b = sum(i[0] * i[1] for i in zip(vec_a, vec_b))\n",
        "  a_2 = sum([i*i for i in vec_a])\n",
        "  b_2 = sum([i*i for i in vec_b])\n",
        "  return a_b/(math.sqrt(a_2) * math.sqrt(b_2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i-iGyPtAvOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tfidf_vector = False\n",
        "tfidf_vector = True\n",
        "\n",
        "if tfidf_vector:\n",
        "  #\n",
        "  # calculate the similiarity based on word tfidf vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"tfidf {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc_tfidf_vec[doc_a], doc_tfidf_vec[doc_b])\n",
        "else:\n",
        "  #\n",
        "  # calculate the similiarity based on word bow vector\n",
        "  #\n",
        "  def doc_similarity(doc_a, doc_b):\n",
        "    # logging.debug(\"bow {}, {}\".format(doc_a, doc_b))\n",
        "    return cosine_similarity(doc2vec(doc_a), doc2vec(doc_b))\n",
        "  \n",
        "if logging.getLogger().getEffectiveLevel() == logging.DEBUG:\n",
        "  seed_doc = corpus_title[5]\n",
        "  logging.debug('> \"{}\"'.format(seed_doc))\n",
        "  similarities = [doc_similarity(seed_doc, doc) for id, doc in enumerate(corpus)]\n",
        "  logging.debug(similarities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-rJBkKQAz1T",
        "colab_type": "text"
      },
      "source": [
        "# Find Similar Documents\n",
        "Find and print the $k$ most similar titles to a given title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LYwyhqBA0ZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def k_similar(seed_id, k):\n",
        "  seed_title = corpus_title[seed_id]\n",
        "  print('> \"{}\"'.format(seed_title))\n",
        "  similarities = [doc_similarity(seed_title, title) for id, title in enumerate(corpus_title)]\n",
        "  logging.debug(\"Similiarities: {}\".format(similarities))\n",
        "  top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i])[-k:] # https://stackoverflow.com/questions/13070461/get-indices-of-the-top-n-values-of-a-list\n",
        "  nearest = [[corpus_title[id], similarities[id]] for id in top_indices]\n",
        "  print()\n",
        "  for story in reversed(nearest):\n",
        "    print('* \"{}\" ({})'.format(story[0], story[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QLn8qkZvt7g",
        "colab_type": "text"
      },
      "source": [
        "# Task 2: Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igRR3rIcA2S2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus_title, corpus = get_corpus('https://raw.githubusercontent.com/bshmueli/108-nlp/master/buzzfeed.csv', 'ALL', to_lower=False, title_only=False)\n",
        "vocab, doc_vocab = get_vocab(corpus)\n",
        "vocab = vocab.most_common(512)\n",
        "vocab_idf = cal_vocab_idf(vocab, doc_vocab)\n",
        "doc_tfidf_vec = cal_doc_tfidf_vec(vocab, doc_vocab, vocab_idf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQzsVcXKZaub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "470b3e52-58a7-4bab-b2e5-37cd1cc3a5c2"
      },
      "source": [
        "#\n",
        "# My Student ID = 0686028\n",
        "#\n",
        "k_similar(686028 % 1000, 5)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"A Man On A Bus Was Seen With A 30 Gallon Tub Of Cinnabon Frosting And People Have So Many Questions\"\n",
            "\n",
            "* \"A Man On A Bus Was Seen With A 30 Gallon Tub Of Cinnabon Frosting And People Have So Many Questions\" (1.0)\n",
            "* \"Hollywood’s Forgotten Gay Romance\" (0.4048519047948762)\n",
            "* \"How The Best Podcast Of The Year Was Made\" (0.38040469572531127)\n",
            "* \"Invasion Of The Big-Brained Sci-Fi Blockbuster!\" (0.30025812091745924)\n",
            "* \"Sean Penn Says He Has “Nothin’ To Hide” Regarding El Chapo Interview\" (0.2750500512155769)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctF0iM9jDgcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "38383139-baed-44d2-fc83-ebf60a72a1a1"
      },
      "source": [
        "#\n",
        "# for the content of (my student id % 1000) is too generic to identify the results, I test some more articles.\n",
        "#\n",
        "k_similar(55, 5)\n",
        "print('\\n\\n')\n",
        "k_similar(33, 5) "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"Zika Is Spreading Fast In Puerto Rico, Could Reach 25% Of Population\"\n",
            "\n",
            "* \"Zika Is Spreading Fast In Puerto Rico, Could Reach 25% Of Population\" (0.9999999999999998)\n",
            "* \"Zika Virus Updates: Few Pregnant Travelers Are Infected With Zika\" (0.9304581159208551)\n",
            "* \"3 Zika Deaths In Venezuela\" (0.8963314154130989)\n",
            "* \"Everything You Need To Know About Zika Virus In 148 Seconds\" (0.8926574979212561)\n",
            "* \"Zika Panic Spreads Among Pregnant Women\" (0.8725727710572433)\n",
            "\n",
            "\n",
            "\n",
            "> \"Snap Inc.’s Growth Is Pissing Off Its Neighbors\"\n",
            "\n",
            "* \"Snap Inc.’s Growth Is Pissing Off Its Neighbors\" (1.0000000000000002)\n",
            "* \"Despite Legal Troubles, This Startup Is Trying To Do Right By Workers\" (0.4435912350072317)\n",
            "* \"This Startup Is Trying To Help Its Workers While Fighting Regulations\" (0.44211342774763224)\n",
            "* \"How High-Flying Zenefits Fell To Earth\" (0.3577373234338207)\n",
            "* \"The Convicted Con Artist Of The Winter White House\" (0.33425852576818466)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_34gtsLacUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}